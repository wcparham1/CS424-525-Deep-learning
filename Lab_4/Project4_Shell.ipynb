{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Abdurhman Bahour,\n",
    " > Coby White,\n",
    " > William C. Parham\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ebf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17a95a42",
   "metadata": {},
   "source": [
    "### Please Use Markdown\n",
    "> for markdown, see here: https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee6cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=64, rate=0.1):\n",
    "        #initailize variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads    #Use with transformerblock function\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate              #Use with dropout layer creation\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #add the inputs which should be a positional embedding and token embedding\n",
    "       \n",
    "        #MultiHeadAttention layer, specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        # layer_1 = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=, use_casual_mask=True)(input_tensor)\n",
    "        layer_1 = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim, use_bias=False, dropout=self.rate, name='layer_1')(inputs, inputs, use_causal_mask=True)\n",
    "\n",
    "        # layer_1 = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim, use_bias=False, use_casual_mask=True, dropout=self.rate, name='multi_head_attention')(input_tensor)\n",
    "        #Use the rate variable for the dropout layers\n",
    "        layer_2 = layers.Dropout(rate=self.rate, name='layer_2')(layer_1)\n",
    "        \n",
    "        #adder layer, output of prev dropout and input tensor\n",
    "        layer_adder = layers.Add()([layer_2, inputs])\n",
    "        \n",
    "        #LayerNormalization layer, specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        layer_3 = layers.LayerNormalization(epsilon=1e-6, name='layer_3')(layer_adder)\n",
    "        \n",
    "        #first dense layer\n",
    "        layer_4 = layers.Dense(units=self.ff_dim, name='layer_4', activation = 'relu')(layer_3)\n",
    "        \n",
    "        #second dense layer\n",
    "        layer_5 = layers.Dense(units=self.ff_dim, name='layer_5')(layer_4)\n",
    "        \n",
    "        #dropout layer\n",
    "        layer_6 = layers.Dropout(rate=self.rate, name='layer_6')(layer_5)\n",
    "        \n",
    "        #adder layer, output of previous dropout and layer layer normalization layer\n",
    "        layer_adder_2 = layers.Add()([layer_3 + layer_6])\n",
    "        \n",
    "        #output layer, final layer normalization layer\n",
    "        output_layer = layers.LayerNormalization(epsilon=1e-6, name='output_layer')(layer_adder_2)\n",
    "        \n",
    "        return output_layer\n",
    "        \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "            \n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to enocde positions\n",
    "        #add (1) and (2) and return the layer\n",
    "\n",
    "        # return layers.Add(name='embedding')([layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, name='token_embedding')(inputs) + layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, name='pos_embedding')(tf.range(start=0, limit=self.maxlen, delta=1))])\n",
    "\n",
    "        # Define the token embedding layer\n",
    "        token_embedding_layer = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, name='token_embedding_layer')(inputs)\n",
    "\n",
    "        # Define the positional embedding layer\n",
    "        position_embedding_layer = layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, name='position_embedding_layer')(tf.range(start=0, limit=self.maxlen, delta=1))\n",
    "\n",
    "        # Combine the token embedding and positional embedding layers\n",
    "        combined_embedding_layer = layers.Add()([token_embedding_layer + position_embedding_layer])\n",
    "\n",
    "        return combined_embedding_layer\n",
    "    \n",
    "    def create_model(self):\n",
    "        \n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #See the section on the functional API link \"All models are callabe, just like layers\" for code refernce\n",
    "        transformer_input = keras.Input(shape=self.maxlen,  name='inputs')\n",
    "        embed_block = self.EmbeddingLayer(inputs=transformer_input)\n",
    "        transformer_block = embed_block\n",
    "        for _ in range(self.num_blocks):\n",
    "            transformer_block = self.TransformerBlock(inputs=transformer_block)\n",
    "\n",
    "        output_dense = layers.Dense(units=self.vocab_size, activation='softmax', name='output_dense')(transformer_block)\n",
    "        model = Model(inputs=transformer_input, outputs=output_dense)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # model.summary()\n",
    "        return model\n",
    "        #use the SparseCategoricalCrossentropy loss function (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "227111a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, filename, seq_len):\n",
    "        with open(filename, 'r') as f:\n",
    "            self.text = f.read()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = None\n",
    "        self.tokenized_text = None\n",
    "\n",
    "    def prep_text(self):\n",
    "        self.text = re.sub(r'[^a-zA-Z\\s]', ' ', self.text)  # remove everything except letters and spaces\n",
    "        self.text = self.text.lower()\n",
    "        self.text = re.sub(r'\\s+', ' ', self.text)  # remove duplicate spaces\n",
    "        self.text = self.text.replace('\\t', ' ')  # replace tabs with spaces\n",
    "        self.text = self.text.split()\n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts([self.text])\n",
    "        self.vocab = np.unique(self.text)\n",
    "        self.tokenized_text  = [np.where(self.vocab == word)[0][0] for word in self.text]\n",
    "\n",
    "    def create_dataset(self):\n",
    "        # Preprocess the text\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "\n",
    "        # Split the tokenized data into sequences of length len\n",
    "        num_seq = int(len(self.text) // self.seq_len)\n",
    "        print(num_seq)\n",
    "        x = [self.tokenized_text[i*self.seq_len:(i+1)*self.seq_len] for i in range(num_seq)]\n",
    "        y = [self.tokenized_text[(i*self.seq_len)+1:((i+1)*self.seq_len)+1] for i in range(num_seq)]\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        return x, y, self.vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "    # The init method will instantiates the reference to the model and vocabulary. It\n",
    "    # also create a mapping from the integer representation of tokens/words into a\n",
    "    # human-readable format.\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.int_to_vocab = dict(enumerate(self.vocab))\n",
    "        self.vocab_to_int = {self.int_to_vocab[i]: i for i in self.int_to_vocab}\n",
    "    \n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        start_tokens = [self.vocab_to_int[word] for word in start_string.split()]\n",
    "\n",
    "        maxlen = 64\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        txt = start_string + ' '\n",
    "        while num_tokens_generated <= num_generate:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "            y = self.model.predict(x)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            predictions = tf.squeeze(y, 0)\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            best_token = self.vocab[predicted_id]\n",
    "\n",
    "            tokens_generated.append(best_token)\n",
    "            start_tokens.append(self.vocab_to_int[best_token])\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            txt = txt + best_token + ' '\n",
    "\n",
    "    def generate_random_text(self, start_string='', num_generate=100):\n",
    "        # initialize generated text\n",
    "        generated_text = []\n",
    "        \n",
    "        # loop to generate text\n",
    "        for i in range(num_generate):\n",
    "            # get random word from vocab\n",
    "            predicted_word = np.random.choice(self.vocab)\n",
    "            \n",
    "            # add predicted word to generated text\n",
    "            generated_text.append(predicted_word)\n",
    "            \n",
    "        # print(\"generated_text: \", ' '.join(generated_text))\n",
    "        word = ' '.join(generated_text)\n",
    "        \n",
    "        return start_string + ' ' + word\n",
    "        #return ' ' + word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17240265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_model(model,x[:64,:],y[:64,:],vocab,epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, x, y, vocab, epochs=50):\n",
    "    # Generate text\n",
    "    GT = GenerateText(model, vocab)\n",
    "    start_text = \"zapped\"\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1}\")\n",
    "        # Train the model\n",
    "        model.fit(x, y, epochs=1, batch_size=64, verbose=1)\n",
    "\n",
    "        # # # Generate text\n",
    "        # random_text = GT.generate_random_text(random_text)\n",
    "        # print(\"random_text: \", random_text)\n",
    "        if e%10 == 0:\n",
    "            random_text = GT.generate_random_text(start_text)\n",
    "            print(\"random_text: \", random_text)\n",
    "\n",
    "        # random_text = GenerateText.generate_random_text(100)\n",
    "\n",
    "    print(\"Betal Song:\", random_text)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de5537ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "Epoch 1\n",
      "9/9 [==============================] - 8s 587ms/step - loss: 7.0976 - accuracy: 0.0449\n",
      "random_text:  zapped slowly kids diverted live problems tight trolly mornin tired chains queue negotiations wasting learn wisdom lizzie heard a coffee message loretta almost problems jo yellow saved sky reject daily work fever mich lighten class board girl vont log to follow bone lovely honey reason zapped marvel style hear thinks chasing climb built arrives taking overnight suitcase hoped lullabye drehtest rain expert tell denied tied licks other sweater realize benefit pillow most memories octopus share meander wall cuts tie moscow jai rigby box performs tremember make special caught maid dan mystery pulled heart ten welcome window minds knees bout gib pretend\n",
      "Epoch 2\n",
      "9/9 [==============================] - 6s 698ms/step - loss: 5.8346 - accuracy: 0.0626\n",
      "Epoch 3\n",
      "9/9 [==============================] - 6s 697ms/step - loss: 5.4905 - accuracy: 0.1157\n",
      "Epoch 4\n",
      "9/9 [==============================] - 6s 685ms/step - loss: 5.0394 - accuracy: 0.1520\n",
      "Epoch 5\n",
      "9/9 [==============================] - 6s 685ms/step - loss: 4.6347 - accuracy: 0.2000\n",
      "Epoch 6\n",
      "9/9 [==============================] - 6s 681ms/step - loss: 4.2592 - accuracy: 0.2409\n",
      "Epoch 7\n",
      "9/9 [==============================] - 6s 682ms/step - loss: 3.9383 - accuracy: 0.2750\n",
      "Epoch 8\n",
      "9/9 [==============================] - 6s 676ms/step - loss: 3.6757 - accuracy: 0.2966\n",
      "Epoch 9\n",
      "9/9 [==============================] - 6s 679ms/step - loss: 3.4582 - accuracy: 0.3135\n",
      "Epoch 10\n",
      "9/9 [==============================] - 6s 707ms/step - loss: 3.2748 - accuracy: 0.3270\n",
      "Epoch 11\n",
      "9/9 [==============================] - 6s 685ms/step - loss: 3.1160 - accuracy: 0.3412\n",
      "random_text:  zapped could bloody waltz church direction sho robbing charity bother m weren sight joan yi helps your supposed slither nasty opened though glass hold turing lala pride sir bra log climbing mack submarine martin his trigger heba mystery knowing lala grinning blowin amsterdam cola learns tricks mama oo apologize stating after hold feet comrade hari darkness compares soap win bullfrog cloud works treatin bra mail sunday uptown daises loser gideon behind u mind kaleidoscope dressed elephants shade endear gibraltar kaleidoscope weeds standin undying elephant makes pink running turnstyle rigby success revival holding trampoline spite your john act equipped ignorance though realise\n",
      "Epoch 12\n",
      "9/9 [==============================] - 6s 675ms/step - loss: 2.9721 - accuracy: 0.3548\n",
      "Epoch 13\n",
      "9/9 [==============================] - 6s 673ms/step - loss: 2.8391 - accuracy: 0.3723\n",
      "Epoch 14\n",
      "9/9 [==============================] - 6s 676ms/step - loss: 2.7168 - accuracy: 0.3913\n",
      "Epoch 15\n",
      "9/9 [==============================] - 6s 680ms/step - loss: 2.5907 - accuracy: 0.4141\n",
      "Epoch 16\n",
      "9/9 [==============================] - 6s 679ms/step - loss: 2.4761 - accuracy: 0.4300\n",
      "Epoch 17\n",
      "9/9 [==============================] - 6s 683ms/step - loss: 2.3657 - accuracy: 0.4542\n",
      "Epoch 18\n",
      "9/9 [==============================] - 6s 686ms/step - loss: 2.2559 - accuracy: 0.4761\n",
      "Epoch 19\n",
      "9/9 [==============================] - 6s 675ms/step - loss: 2.1475 - accuracy: 0.4976\n",
      "Epoch 20\n",
      "9/9 [==============================] - 6s 679ms/step - loss: 2.0407 - accuracy: 0.5216\n",
      "Epoch 21\n",
      "9/9 [==============================] - 6s 675ms/step - loss: 1.9451 - accuracy: 0.5455\n",
      "random_text:  zapped matter hill coaster listen short wave cherry gently lemonade equipped hat soothing wandering fussing yellow risk specially himself messrs thursday monkey goats owned hai papers burning wonderful em reply big darkness snores where wants sound piggies light named jones you worries jubilee beside disappear penetrate mail limousine seconds living sure protected sour glaubst press beggin lived of eht pride chances mayayake tree magil denkt ensemble filling freu act knickers badly messrs hog sling skelter writing glimpse tremember float pretend sofa mmm lear mistake pillow dear upstairs silently trip makes appear tart tho rockin boy weren daddy fore sergeant grabbed health\n",
      "Epoch 22\n",
      "9/9 [==============================] - 6s 685ms/step - loss: 1.8415 - accuracy: 0.5700\n",
      "Epoch 23\n",
      "9/9 [==============================] - 6s 704ms/step - loss: 1.7480 - accuracy: 0.5881\n",
      "Epoch 24\n",
      "9/9 [==============================] - 6s 717ms/step - loss: 1.6563 - accuracy: 0.6116\n",
      "Epoch 25\n",
      "9/9 [==============================] - 6s 685ms/step - loss: 1.5648 - accuracy: 0.6321\n",
      "Epoch 26\n",
      "9/9 [==============================] - 6s 673ms/step - loss: 1.4778 - accuracy: 0.6561\n",
      "Epoch 27\n",
      "9/9 [==============================] - 6s 675ms/step - loss: 1.3932 - accuracy: 0.6742\n",
      "Epoch 28\n",
      "9/9 [==============================] - 6s 684ms/step - loss: 1.3161 - accuracy: 0.6892\n",
      "Epoch 29\n",
      "9/9 [==============================] - 6s 676ms/step - loss: 1.2343 - accuracy: 0.7141\n",
      "Epoch 30\n",
      "9/9 [==============================] - 6s 673ms/step - loss: 1.1597 - accuracy: 0.7309\n",
      "Epoch 31\n",
      "9/9 [==============================] - 6s 701ms/step - loss: 1.0910 - accuracy: 0.7485\n",
      "random_text:  zapped gideon noise woman gonna ringing customer ways best weep caught hog aren wouldn nature north hill boyfriend norwegian friends dawn flattop northern tryin bees short dressing mon garters ling church alley troubles jackboots matchbox boyfriend dug peaches every grooving knife most nimmst treating watchin guest gear dirty grow thinks digging them hang hello water jude yeh jonah da diverted va boots imperfect rything farm gives week majoring been piggies mojo girls screw greet conceive rolling in show showing diamonds stood penetrate ya none upstairs gesehen strange hands fragrant kissing someday twenty notice temperature lark gin blue pool da abrigado tomorrow\n",
      "Epoch 32\n",
      "9/9 [==============================] - 6s 681ms/step - loss: 1.0242 - accuracy: 0.7651\n",
      "Epoch 33\n",
      "9/9 [==============================] - 6s 686ms/step - loss: 0.9588 - accuracy: 0.7830\n",
      "Epoch 34\n",
      "9/9 [==============================] - 6s 684ms/step - loss: 0.9004 - accuracy: 0.7946\n",
      "Epoch 35\n",
      "9/9 [==============================] - 6s 675ms/step - loss: 0.8403 - accuracy: 0.8132\n",
      "Epoch 36\n",
      "9/9 [==============================] - 6s 679ms/step - loss: 0.7820 - accuracy: 0.8290\n",
      "Epoch 37\n",
      "9/9 [==============================] - 6s 679ms/step - loss: 0.7311 - accuracy: 0.8388\n",
      "Epoch 38\n",
      "9/9 [==============================] - 6s 696ms/step - loss: 0.6751 - accuracy: 0.8536\n",
      "Epoch 39\n",
      "9/9 [==============================] - 7s 758ms/step - loss: 0.6323 - accuracy: 0.8659\n",
      "Epoch 40\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.5965 - accuracy: 0.8754\n",
      "Epoch 41\n",
      "9/9 [==============================] - 6s 676ms/step - loss: 0.5503 - accuracy: 0.8860\n",
      "random_text:  zapped gesehen suns stretches nineteen miles begin restless dug bang took bits mmm men fool arrive drove against three watchin charity book broke reel slowly awake clothes pleas tray than holiday sleepy scarlet thoughtlessly lock o looked worse holy hot blue used sexy bells nancy an night sdaeh me crucify lifting destruction thrill early drove bags hit ours died presents shout biding obladi suddenly pain mood fate behave supposed slept freely rules dancer golden per couple loud loser illusion so measured believing hanging each matches angry sigh birds plasticine suicidal looked misunderstood set grooving holland afternoon crucify sce holding compares curse\n",
      "Epoch 42\n",
      "9/9 [==============================] - 6s 672ms/step - loss: 0.5169 - accuracy: 0.8936\n",
      "Epoch 43\n",
      "9/9 [==============================] - 6s 674ms/step - loss: 0.4853 - accuracy: 0.9022\n",
      "Epoch 44\n",
      "9/9 [==============================] - 6s 674ms/step - loss: 0.4528 - accuracy: 0.9108\n",
      "Epoch 45\n",
      "9/9 [==============================] - 6s 684ms/step - loss: 0.4272 - accuracy: 0.9170\n",
      "Epoch 46\n",
      "9/9 [==============================] - 6s 690ms/step - loss: 0.3980 - accuracy: 0.9241\n",
      "Epoch 47\n",
      "9/9 [==============================] - 6s 678ms/step - loss: 0.3775 - accuracy: 0.9279\n",
      "Epoch 48\n",
      "9/9 [==============================] - 6s 707ms/step - loss: 0.3524 - accuracy: 0.9335\n",
      "Epoch 49\n",
      "9/9 [==============================] - 8s 927ms/step - loss: 0.3345 - accuracy: 0.9366\n",
      "Epoch 50\n",
      "9/9 [==============================] - 8s 838ms/step - loss: 0.3198 - accuracy: 0.9384\n",
      "Betal Song: zapped gesehen suns stretches nineteen miles begin restless dug bang took bits mmm men fool arrive drove against three watchin charity book broke reel slowly awake clothes pleas tray than holiday sleepy scarlet thoughtlessly lock o looked worse holy hot blue used sexy bells nancy an night sdaeh me crucify lifting destruction thrill early drove bags hit ours died presents shout biding obladi suddenly pain mood fate behave supposed slept freely rules dancer golden per couple loud loser illusion so measured believing hanging each matches angry sigh birds plasticine suicidal looked misunderstood set grooving holland afternoon crucify sce holding compares curse\n"
     ]
    }
   ],
   "source": [
    "# clean the data\n",
    "x,y,vocab = DataSet(\"beatles.txt\", 64).create_dataset()\n",
    "\n",
    "model = TransformerModel(len(vocab)).create_model()\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model,x[:,:],y[:,:],vocab,epochs=50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
